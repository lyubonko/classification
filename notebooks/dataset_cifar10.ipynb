{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The CIFAR-10 dataset**\n",
    "\n",
    "*source*: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "*description*: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do in the terminal: \n",
    "\n",
    "1. go the folder 'data' where assigments are \n",
    "<pre> > cd ../data </pre>\n",
    "2. get the data and perform train/test partition [YOU NEED ~ 300 MB] \n",
    "<pre> > chmod +x get_cifar10_dataset.sh </pre>\n",
    "<pre> > ./get_cifar10_dataset.sh </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the associated webpage in a new window\n",
    "import IPython\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar.html'\n",
    "iframe = '<iframe src=' + url + ' width=\"100%\" height=500></iframe>'\n",
    "IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch stuff\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = \"../src\"\n",
    "sys.path.append(path_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local \n",
    "from datasets.ds_cifar10_load import load_CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make interactive plotting possible\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "path_data = \"../data/cifar10\"\n",
    "cifar10_dir = os.path.join(path_data, 'cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \" + str(len(classes)) + \" classes:\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_im = np.random.randint(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = X_train[indx_im,:,:,:]\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(im.astype('uint8'))\n",
    "plt.title(\"[class]: %s\" % classes[y_train[indx_im]]);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, size, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = y_train.shape[0]\n",
    "n_test = y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look more closely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> You are invited to play with code below  </span>  \n",
    "\n",
    "<span style=\"color:red\"> - change 'samples_per_class' </span>  \n",
    "<span style=\"color:red\"> - make different runs, you should see different images </span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = 4\n",
    "classes_to_see = np.arange(0,10)\n",
    "for cls in classes_to_see:\n",
    "    class_images = np.random.choice(np.arange(n_train)[y_train == cls], samples_per_class, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(1 * samples_per_class, 2))    \n",
    "    for i, indx_im in enumerate(class_images):\n",
    "        plt_idx = i + 1\n",
    "        plt.subplot(1, samples_per_class, plt_idx)\n",
    "        \n",
    "        im = X_train[indx_im,:,:,:].astype('uint8')\n",
    "        \n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.text(-20, 16, classes[cls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in Computer Vision to make different image transforms.  \n",
    "*pytorch* (*torchvision* to be more precise) provides several commonly used transforms and tools to combine these transforms.\n",
    "\n",
    "Please visit for details\n",
    "https://github.com/pytorch/vision#transforms\n",
    "\n",
    "Let's check different transformations\n",
    "\n",
    "First we define useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(tensor_):\n",
    "    \"\"\" Bring the tensor back to image\"\"\"\n",
    "    return transforms.ToPILImage()(tensor_)\n",
    "\n",
    "def display_diff(im, im_transformed):\n",
    "    \"\"\" Display a difference between original image and transformed\"\"\"\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('original')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(im_transformed)\n",
    "    plt.title('transformed') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pick an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_im = np.random.randint(50000)\n",
    "im = X_train[indx_im,:,:,:] \n",
    "# 'im' now is a numpy array, will make it an image (PIL.image)\n",
    "print(type(im))\n",
    "im = Image.fromarray(im.astype('uint8'))\n",
    "print(type(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define transform itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(20),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words we define the following transformation:  \n",
    "1. crop the image at the center to have a region of the given size (20 in our case)\n",
    "2. randomly horizontally flips the given image with a probability of 0.5 (probability by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply transform and check the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transform\n",
    "im_transformed = transform(im)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **[PROBLEM I]**: </span> \n",
    "\n",
    "<span style=\"color:red\">Define the following transform </span>  \n",
    "<span style=\"color:red\">1. crop the image at a random location to have a region of the given size (24)</span>  \n",
    "<span style=\"color:blue\">use 'RandomCrop'    https://github.com/pytorch/vision#randomcropsize-padding0</span>    \n",
    "<span style=\"color:red\">2. randomly horizontally flip the given image with a probability of 0.5 </span>  \n",
    "<span style=\"color:red\">3. rescale the input image to the given 'size' (32) </span>  \n",
    "<span style=\"color:blue\"> consider to use 'Scale' https://github.com/pytorch/vision#scalesize-interpolationimagebilinear </span>  \n",
    "\n",
    "<span style=\"color:red\"> apply this transform and see the results </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(24),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Scale(32)\n",
    "])\n",
    "# apply transform\n",
    "im_transformed = transform(im)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting tranformation is normalization, commonly used to normalize an image, prior to training  \n",
    "It operates on a Tensor rather than an image and requires two params - mean & std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_mean = [0.5, 0.5, 0.5]\n",
    "im_std = [0.3, 0.3, 0.3]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=im_mean, std=im_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transform\n",
    "tensor_transformed = transform(im)\n",
    "# convert tensor to image\n",
    "im_transformed = tensor2im(tensor_transformed)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **[PROBLEM II]**: </span> \n",
    "\n",
    "<span style=\"color:red\">Implement the function, which makes inverse transformation</span>\n",
    "\n",
    "<span style=\"color:blue\">look at realization of 'normalize' transform in</span>   https://github.com/pytorch/vision/blob/master/torchvision/transforms.py#L129  \n",
    "<span style=\"color:blue\"> consider to use in-place version of tensor operations as in the example of 'normalize' transform above </span>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor, mean, std):    \n",
    "    \"\"\"\n",
    "    Make inverse transform to 'normalize'\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor to unnormalize\n",
    "        mean (sequence)      : Sequence of means for R, G, B channels respectively.\n",
    "        std (sequence)       : Sequence of standard deviations for R, G, B channels respectively.  \n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: unnormalized tensor.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with datasets pytorch provides useful abstractions, like *dataset* and *dataloder* [https://github.com/pytorch/vision#datasets].  \n",
    "There are \n",
    "* prepared datasets, like MNIST, CIFAR10 and CIFAR100, COCO, etc.\n",
    "* *ImageFolder* dataset, which allows you to cook dataset for yourself without much efforts.\n",
    "\n",
    "\n",
    "The former is used here for Cifar10 dataset.\n",
    "The later is especially useful when working with new data.\n",
    "\n",
    "On top of the *dataset* there is *dataloader*.\n",
    "The *dataloader* is used, as name suggests, to load the data.  \n",
    "It does that efficiently, with multi-threading, so you should not worry about how to feed you model with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at src/data_set.py where the *DataSetCifar10* is defined.  \n",
    "There train & test dataloaders are bundled together, for convinence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.ds_cifar10 import DataSetCifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = DataSetCifar10(path_data,\n",
    "                      batch_size_train=64,\n",
    "                      batch_size_val=64,\n",
    "                      download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_set.loader['train'])\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print (images.size())\n",
    "print (labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_set.loader['val'])\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print (images.size())\n",
    "print (labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
