{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describable Textures Dataset (DTD)**  \n",
    "*source*: https://www.robots.ox.ac.uk/~vgg/data/dtd/  \n",
    "*description*: DTD is a texture database, consisting of 5640 images, organized according to a list of 47 terms (categories) inspired from human perception. There are 120 images for each category. Image sizes range between 300x300 and 640x640.  \n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do in the terminal:\n",
    "\n",
    "1. go the folder 'data' where assigments are \n",
    "<pre> > cd ../data </pre>  \n",
    "\n",
    "2. get the data and perform train/test partition [YOU NEED ~ 700 MB] \n",
    "<pre> > chmod +x get_dtd_dataset.sh </pre>  \n",
    "<pre> > ./get_dtd_dataset.sh </pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the associated webpage\n",
    "import IPython\n",
    "url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/'\n",
    "iframe = '<iframe src=' + url + ' width=\"100%\" height=500></iframe>'\n",
    "IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libs\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch stuff\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = \"../src\"\n",
    "sys.path.append(path_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make interactive plotting possible\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "path_data = '../data/dtd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(os.path.join(path_data, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \" + str(len(classes)) + \" classes:\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, size, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in classes[:5]:\n",
    "    for train_test in ['train', 'test']:\n",
    "        path_dir_cls = os.path.join(path_data, train_test, cls)\n",
    "        imgs_list = os.listdir(path_dir_cls)\n",
    "        print( \"[%s] %s : %d\" % (train_test, cls, len(imgs_list)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes have the same amount of images 120, 80 images in train and 40 images in test.\n",
    "  \n",
    "So we have $ (80 + 40) * 47 = 5640$ images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "width  [max, min]:  [271, 900]\n",
    "height [max, min]:  [231, 778]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main message is that the image sizes are different and we have to deal with it when we will train NN on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_cls = np.random.randint(len(classes))\n",
    "cls = classes[indx_cls]\n",
    "\n",
    "class_images = os.listdir(os.path.join(path_data, 'train', cls))\n",
    "indx_im = np.random.randint(len(class_images))\n",
    "\n",
    "im_name = class_images[indx_im]\n",
    "im_full_name = os.path.join(path_data, 'train', cls, im_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(im_full_name)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(im)\n",
    "plt.title(\"[class]: \" + cls + \"\\n [im]: \" + im_name);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look more closely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = 7\n",
    "classes_to_see = np.arange(20,28)\n",
    "#classes_to_see = [1,2,3]\n",
    "train_test = \"train\"\n",
    "#train_test = \"test\"\n",
    "for cls in [classes[i] for i in classes_to_see]:\n",
    "    class_images = os.listdir(os.path.join(path_data, train_test, cls))\n",
    "    class_images_pick = np.random.choice(class_images, samples_per_class, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(2 * samples_per_class, 5))    \n",
    "    for i, im_name in enumerate(class_images_pick):\n",
    "        plt_idx = i + 1\n",
    "        plt.subplot(1, samples_per_class, plt_idx)\n",
    "        im_full_name = os.path.join(path_data, train_test, cls, im_name)\n",
    "        im = Image.open(im_full_name)\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            im_x, im_y = im.size\n",
    "            plt.text(-400, im_y / 2, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in Computer Vision to make different image transforms.  \n",
    "*pytorch* (*torchvision* to be more precise) provides several commony used transforms and tools to combine these transforms.\n",
    "\n",
    "Plese visit for details\n",
    "https://github.com/pytorch/vision#transforms\n",
    "\n",
    "Let check different transformation\n",
    "\n",
    "First we define useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(tensor_):\n",
    "    \"\"\" Bring the tensor back to image\"\"\"\n",
    "    return transforms.ToPILImage()(tensor_)\n",
    "\n",
    "def display_diff(im, im_transformed):\n",
    "    \"\"\" Display a difference between original image and transformed\"\"\"\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('original')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(im_transformed)\n",
    "    plt.title('transformed')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dir = os.path.join(path_data, 'train', classes[8])\n",
    "im_full_name = im_dir + \"/\" + os.listdir(im_dir)[3]\n",
    "im = Image.open(im_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define transform itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words we define the following transformation:  \n",
    "1. crop the image at the center to have a region of the given size (224 in our case)\n",
    "2. randomly horizontally flips the given image with a probability of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply transform and check the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transform\n",
    "im_transformed = transform(im)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Scale(32)\n",
    "])\n",
    "# apply transform\n",
    "im_transformed = transform(im)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting tranformation is normalization, commonly used to normalize an image, prior to training  \n",
    "It operates on a Tensor, rather than an image and requires two params - mean & std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transform\n",
    "tensor_transformed = transform(im)\n",
    "# convert tensor to image\n",
    "im_transformed = tensor2im(tensor_transformed)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor, mean, std):    \n",
    "    \"\"\"\n",
    "    Make inverse transform to 'normalize'\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor to unnormalize\n",
    "        mean (sequence)      : Sequence of means for R, G, B channels respectively.\n",
    "        std (sequence)       : Sequence of standard deviations for R, G, B channels respectively.  \n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: unnormalized tensor.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">when running the following cell you have to see identical images</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_back = tensor2im(unnormalize(transform(im), imagenet_mean, imagenet_std))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im)\n",
    "plt.axis('off');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_back)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with datasets pytorch provides useful abstractions, like *dataset* and *dataloder* [https://github.com/pytorch/vision#datasets].  \n",
    "There are \n",
    "* prepared datasets, like MNIST, CIFAR10 and CIFAR100, COCO, etc.\n",
    "* *ImageFolder* dataset, which allows you to cook dataset for yourself without much efforts.\n",
    "\n",
    "\n",
    "The later is used here for DTD dataset and is especially useful when working with new data.\n",
    "\n",
    "On top of the *dataset* there is *dataloader*.\n",
    "The *dataloader* is used, as name suggests, to load the data.  \n",
    "It does that efficiently, with multi-threading, so you should not worry about how to feed you model with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at src/data_set.py where the *DataSetDTD* is defined.  \n",
    "There train & test dataloaders are bundled together, for convinence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.ds_dtd import DataSetDTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = DataSetDTD(path_data,\n",
    "                      batch_size_train=64,\n",
    "                      batch_size_val=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load dataset from files.\n",
    "data_iter = iter(data_set.loader['train'])\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print (images.size())\n",
    "print (labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load dataset from files.\n",
    "data_iter = iter(data_set.loader['val'])\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print (images.size())\n",
    "print (labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
